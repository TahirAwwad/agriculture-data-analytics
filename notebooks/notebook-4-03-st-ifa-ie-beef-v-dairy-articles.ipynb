{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c20065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swazy\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "C:\\Users\\Swazy\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import Label\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from nltk.corpus import stopwords\n",
    "from plotly.offline import iplot\n",
    "from pprint import pprint\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "import csv\n",
    "import cufflinks\n",
    "import emoji\n",
    "import en_core_web_sm\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import pyLDAvis\n",
    "import re\n",
    "import scattertext as st\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import spacy\n",
    "import warnings \n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "\n",
    "output_notebook()\n",
    "pd.options.display.max_columns = 30\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9885947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Swazy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd57980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf36ee65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 9, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./../assets/ifa-ie-all-articles.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    605\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    607\u001b[0m )\n\u001b[0;32m    608\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:468\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:1057\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1056\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m-> 1057\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col_dict:\n\u001b[0;32m   1061\u001b[0m             \u001b[38;5;66;03m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:2061\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2060\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2061\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   2063\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mpandas\\_libs\\parsers.pyx:756\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\parsers.pyx:771\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\parsers.pyx:827\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\parsers.pyx:1951\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 9, saw 9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../assets/ifa-ie-all-articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c04f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43251 entries, 0 to 43250\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       43251 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 338.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_scorer(df):\n",
    "    '''Compute vaderSentiment scores for each tweet\n",
    "    Args: Dataframe containing a 'text' column\n",
    "    Returns: Dataframe of vader scores\n",
    "    '''\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vader_scores = df.loc[:,'text'].map(analyzer.polarity_scores)\n",
    "\n",
    "    dvec = DictVectorizer()\n",
    "    vader_scores = dvec.fit_transform(vader_scores)\n",
    "    \n",
    "    vader_scores = pd.DataFrame(vader_scores.toarray(),columns=dvec.feature_names_)\n",
    "    return vader_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_scores = vader_scorer(df)\n",
    "df = pd.concat([df,vader_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba393f0",
   "metadata": {},
   "source": [
    "Hypothesis 1:\n",
    "Null Hypothesis is that sentiment for the beef and the dairy was the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de13f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,4))\n",
    "sns.violinplot(y='compound', x='trend', data=df[tweets.topical_orgs != ''], ax=ax[0])\n",
    "sns.boxplot(y='compound', x='trend', data=df[tweets.topical_orgs != ''], ax=ax[1])\n",
    "plt.tight_layout()\n",
    "show.plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('StDev of beef sentiment',np.std(df[df.trend == 'beef']['compound']))\n",
    "print('StDev of NHS sentiment',np.std(df[df.trend == 'dairy']['compound']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13915a5",
   "metadata": {},
   "source": [
    "Running SciPy's t-test:\n",
    "\n",
    "Note: had sample standard deviations not been equal, then set equal_var = False to use Welch's t-statistic (ie. out standard error is calculated differently because we can't pool our two distributions together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df[df.trend == 'beef']['compound'],\n",
    "                df[df.trend == 'dairy']['compound'], equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0122ad7",
   "metadata": {},
   "source": [
    "Alternatively could have run Statsmodel API's ztest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ed666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.ztest(df[df.trend == 'beef']['compound'],\n",
    "               df[df.trend == 'dairy']['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02ed51",
   "metadata": {},
   "source": [
    "So p-value is essentially 0, which says that if null hypothesis is assumed to be true, then there is 0 (or very small) chance of observing what we've just observed as the alternate hypothesis.\n",
    "How small the p-value is, assuming proper statistical process, is how much confidence we have in rejecting the null hypothesis. Because we're saying there's no chance we'd have seen this alternate hypothesis (so far into the tail as it was) if the null was true.\n",
    "We can construct a 95% confidence interval for our difference in sample means to further confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea420cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bf = np.mean(df[df.trend == 'beef']['compound'])\n",
    "p_dr = np.mean(df[df.trend == 'dairy']['compound'])\n",
    "\n",
    "num_bf = len(df[df.trend == 'beef'])\n",
    "num_dr = len(df[df.trend == 'dairy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53034031",
   "metadata": {},
   "source": [
    "The s.e. for each population (as we're comparing MEANS here) is simply: sigma / sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62cdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_bf = np.std(df[df.trend == 'beef']['compound']) / np.sqrt(num_bf)\n",
    "se_dr = np.std(df[df.trend == 'dairy']['compound']) / np.sqrt(num_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5b213",
   "metadata": {},
   "source": [
    "Alternatively, statsmodels has a function s.e. of the mean of a distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('beef sentiment s.e.', stats.sem(df[df.trend == 'beef']['compound'], axis=None))\n",
    "print('dairy sentiment s.e.', stats.sem(df[df.trend == 'dairy']['compound'], axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90347",
   "metadata": {},
   "source": [
    "With the standard error for both populations to be used in CI formula below: SE(1,2) = SQR(SE1^2 + SE2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_diff = np.sqrt(se_br**2 + se_dr**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = p_bf - p_dr\n",
    "lcb = diff - (1.96 * se_diff)\n",
    "ucb = diff + (1.96 * se_diff)\n",
    "(lcb, ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e4442",
   "metadata": {},
   "source": [
    "Visualise sentiment\n",
    "Can we visualise what type of sentiment ifi had for the beef vs the dairy? Let's look at the words that were being used to better understand how the ifi described each entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d294ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "beef_text = \" \".join(art for art in df.text[df.trend=='beef'])\n",
    "dairy_text = \" \".join(art for art in df.text[df.trend=='dairy'])\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['http', 'https', 'www', 'amp', 'ly', 'bit'])\n",
    "\n",
    "gov_wordcloud = WordCloud(stopwords=stopwords).generate(gov_text)\n",
    "nhs_wordcloud = WordCloud(stopwords=stopwords).generate(nhs_text)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(10,10))\n",
    "ax[0].imshow(gov_wordcloud)\n",
    "ax[0].set_title('beef')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(nhs_wordcloud)\n",
    "ax[1].set_title('dairy')\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
